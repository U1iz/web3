## 我们实际上如何更新权重_推导过程（待完善）

**当误差变化时，相关链接的权重有何变化**

[我们实际上如何更新权重_推导过程](./我们实际上如何更新权重_推导过程.html)

---

$$
\bold{j，k}分别代表隐藏层和输出层的行。例：W_{j,k}代表隐藏层第j个节点链接到输出层第k个节点的权重
$$

$$
t_{k}是期望输出，o_{k}是实际输出
$$

---


$$
\frac{\part E}{\part W_{j,k}}
$$

首先展开误差函数，这是对目标值和实际值之差的平方进行求和，

这里是针对一共n个输出节点的和。
$$
\large\frac{\part E}{\part W_{j,k}} = \frac{\part}{\part W_{j,k}}\sum_{n}(t_{n} - o_{n})^{2}
$$

$$
\part E与\part W_{j,k}的关系为\frac{目标值与实际值之差的平方}{\part W_{j,k}的权重}
$$

$$
因为在节点n的输出O_n只取决于连接到这个节点的链接，即节点k的输出不依赖于权重W_{j,b}，因此可以简化以上表达式得到：
$$


$$
\large\frac{\part E}{\part W_{j,k}} =\bold{} \frac{\part}{\part W_{j,k}}(t_{k} - o_{k})^{2}
$$

---


$$
如果 f = y^2;y=x
$$
$$
那么根据幂规则：\frac{\part f}{\part y} = 2y
$$
$$
根据链式法则：\frac{\part f}{\part x}=\frac{\part f}{\part y}·\frac{\part y}{\part x}
$$
根据链式法则
$$
\large E = (t_k - o_k)^2
\\误差 = (目标值 - 实际值)^2
$$

$$
\large O_k = sigmoid(\sum_j W_{j,k}·O_j)
\\输出结果 = 激活函数(链接权重jk * 隐藏层j行输出)
$$

$$
因为W_{j,k}影响了O_k，O_k又影响了误差E，因此综上所述
$$

$$
\large\frac{\part E}{\part W_{j,k}} = \frac{\part E}{\part O_k}·\frac{\part O_k}{\part W_{j,k}}
$$

---

进一步展开式子
$$
第一项: \frac{\part E}{\part O_k} = \frac{\part}{\part O_k}(t_k - o_k)^2 = -2(t_k - o_k)
$$

$$
第二项: \frac{\part E}{\part W_{j,k}} = \frac\part{\part W_{j,k}}sigmoid(\sum_jWw_{j,k} \cdot O_j)
$$

组合得到

$$
\frac{\part E}{\part W_{j,k}} = -2(t_k - o_k) \cdot \frac\part{\part w_{j,k}}sigmoid(\sum_jW_{j,k} \cdot O_j)
$$


微分S函数：
$$
\frac \part{\part x}sigmoid(x) = sigmoid(x)(1 - sigmoid(x))
$$
代入得到：
$$
\begin{split}
\frac{\part E}{\part W_{j,k}}
&= -2(t_k - o_k) \,\cdot\, \frac\part{\part W_{j,k}}sigmoid(\sum_jW_{j,k} \cdot O_j)(1 - sigmoid(\sum_jW_{j,k}))\,\cdot\, \frac\part{\part W_{j,k}}(\sum_jW_{j,k}\cdot O_j) \\
&= -2(t_k - o_k)\,\cdot\, sigmoid(\sum_j W_{j,k} \cdot O_j)(1 - sigmoid(\sum_j W_{j,k}\cdot O_j)) \cdot O_j
\end{split}
$$
最后一项 o_j 是因为在sigmoid函数内部的表达式也需要看对W_j,k 进行微分，因此再次对S函数应用链式法则，得到的 o_j

---

最终，我们去掉表达式中的常数2，得到用于描述误差函数的斜率，可以以此来调整权重W_j,k了
$$
\frac{\part E}{\part W_{j,k}} = -(t_k - o_k)\;\cdot\; sigmoid(\sum_jW_{j,k}\,\cdot\, O_j)(1 - sigmoid(\sum_jW_{j,k}\,\cdot\, O_j))\;\cdot\; O_j
$$

> 我们还需要做最后一件事。我们所得到的的这个表达式，是为了优化隐藏层和输出层之间的权重。
>
> 现在，我们需要完成工作，为输入层和隐藏层之间的权重找到类似的误差斜率
>
> - 第一部分的（目标值 - 实际值）误差，现在变成了隐藏层节点中重组的向后传播误差，正如在前面所看到的的那样。我们称之为e_j。
> - sigmoid部分可以保持不变，但是内部的求和表达式指的是前一层，因此求和的范围时所有由权重调节的进入隐藏层节点j的输入。我们可以称之为i。
> - 现在，最后一部分是第一层节点的输出o_i。
>
> 这种巧妙的方法，简单利用问题中的对称性构建了一个新的表达式，避免了 大量的工作。
>
> 因此，我们一直在努力达成的最终答案的第二部分如下所示：

$$
\large \frac{\part E}{\part W_{i,j}} = -e_j \;\cdot\; sigmoid(\sum_iW_{i,j}\,\cdot\, O_i)(1-sigmoid(\sum_iW_{i,j}\,\cdot\,O_i)\;\cdot\; O_i
$$

> 现在，我们得到了关于斜率的所有关键的表达式，可以使用这些达表示，在应用每层训练样本后，更新权重。



权重改变的方向与梯度方向相反。我们使用学习因子，调节变化。

当我们建立线性分类器，作为避免被错误的训练样本拉得太远的一种方式，同时为了保证权重不会由于持续的超调而在最小值附近来回摆动。
$$
\huge\bold{new\ W_{j,k} = old\ W_{j,k} - \alpha\;\cdot\;\frac{\part E}{\part W_{j,k}}}
$$

> 公式中的α就是一个因子，用于调节这些变化的强度，确保不会超调。
>
> 我们通常称这个因子为**学习率**。

> 这个表达式不仅适用于隐藏层和输出层之间的权重，而且适用于输入层和隐藏层之间的权重。
>
> 插值就是误差梯度，我们可以使用上述两个表达式来计算这个误差梯度。

利用矩阵展示运算中的元素
$$
\large
\begin{pmatrix}
\Delta W_{1.1} & \Delta W_{2.1} & \Delta W_{3.1} & … \\
\Delta W_{1.2} & \Delta W_{2.2} & \Delta W_{3.2} & … \\
\Delta W_{1.3} & \Delta W_{2.3} & \Delta W_{3.3} & … \\
… & … & … & …
\end{pmatrix}
=
\begin{pmatrix}
E_1 \cdot S_1(1-S_1)\\
E_2 \cdot S_2(1-S_2)\\
E_3 \cdot S_3(1-S_3)\\
…
\end{pmatrix}
\quad\cdot\quad
\begin{pmatrix}
O_1 & O_2 & O_3 & …
\end{pmatrix}\\

\LARGE 权重 = 下一层的值\cdot\LARGE前一层的值
$$

> 由于学习率是一个常数，并没有真正改变如何组织矩阵乘法，因此学习率α可以忽略。

> 仔细观察以上矩阵会发现，表达式的最后一部分，也就是单行的水平矩阵，是前一层 o_j 输出的转置。
>
> 颜色标记显示点乘是正确的方式。

因此权重更新矩阵有如下的矩阵形式，这种形式可以让我们通过计算机编程语言高效地实现矩阵计算。

<img src="我们实际上如何更新权重_推导过程.assets\image-20230610213348986.png" alt="image-20230610213348986" style="zoom:33%;" />

由于我们简化了节点输出 o_k，那些 sigmoids 就消失了。

至此任务完成。



### 偏导数的定义

$$
偏导数（Partial Application）是在有多个变量的函数中，对其中一个变量求导时，将其他变量看作常数，所得到的导数。
$$

$$
在数学中，偏导数可以用符号\frac{\partial f}{\partial x}表示，表示函数f对变量x的偏导数。
$$



偏导数的定义与普通导数的定义类似，但是普通导数是对单个变量的函数进行求导，而偏导数是对多个变量的函数进行求导。
$$
例如，对于函数f(x,y) = x^{2} + y^{2}，我们可以求出它对变量x和变量y的偏导数，分别为：
$$

$$
\frac{\part f}{\part x} = 2x
$$

$$
\frac{\part x}{\part y} = 2y
$$


$$
这表示当我们对变量x求偏导数时，将y视为常量并对x求导；对于变量y求偏导数时，将x视为常量并对y求导。
$$

$$
通过求偏导数，我们可以了解多个变量对函数值的影响，在机器学习和深度学习中，偏导数被广泛应用于损失函数的梯度计算，优化算法的更新等方面。
$$

