## 02_神经网络如何工作

[TOC]

### 1.9 使用矩阵乘法的三层神经网络示例

<img src="02_神经网络如何工作.assets\image-20230528021143475.png" alt="image-20230528021143475" style="zoom:33%;" />

上图展示了具有3层、每层具有三个节点的神经网络示例，并标注了部分链接的权重

> 第一层为输入层，中间层称之为 **隐藏层** ，最后的是输出层



<img src="02_神经网络如何工作.assets\image-20230528021516825.png" alt="image-20230528021516825" style="zoom:33%;" />根据三个输入值，可以得出矩阵 ***I***

接下来我们可以作出由 **输入层到隐藏层** & **隐藏层到输出层** 的权重矩阵 ***w***



<img src="02_神经网络如何工作.assets\image-20230528055815853.png" alt="image-20230528055815853" style="zoom:33%;" /><img src="02_神经网络如何工作.assets\image-20230528055732932.png" alt="image-20230528055732932" style="zoom:33%;" />



```
W = [
	[W1.1, W2.1, W3.1],
	[W1.2, W2.2, W3.2],
	[W1.3, W2.3, W3.3]
]
```

- ***W<span style="font-size: 0.5em;">input_hidden</span>*** 即输入层与隐藏层之间的权重
- ***W<span style="font-size: 0.5em;">hidden_output</span>*** 即隐藏层与输出层之间的权重



接下来就可以开始运算 **隐藏层** 的输入值了

<img src="02_神经网络如何工作.assets\image-20230528063037954.png" alt="image-20230528063037954" style="zoom:33%;" />***X<span style="font-size: 0.5em;">hidden</span>*** = ***W<span style="font-size: 0.5em;">hidden</span>*** · ***I***



<img src="02_神经网络如何工作.assets\image-20230528063326159.png" alt="image-20230528063326159" style="zoom:33%;" />接着再将 ***X<span style="font-size: 0.5em;">hidden</span>*** 代入 **S函数**，最终得到前馈输入信号

> 你会看到 **S函数** 的值域为 **[0, 1]**，如果你回头看看逻辑函数的图形，也可以直观地看到这一点

```js
function f(x) {
    let y = 1 / (1 + 2.71828 ** -x)
    console.log(y.toFixed(4));
}
for (let i = -1000; i < 1000; i++) {
    f(i)
}
```

```
fn.htm:18 0.0000 (991)
fn.htm:18 0.0001
fn.htm:18 0.0003
fn.htm:18 0.0009
fn.htm:18 0.0025
fn.htm:18 0.0067
fn.htm:18 0.0180
fn.htm:18 0.0474
fn.htm:18 0.1192
fn.htm:18 0.2689
fn.htm:18 0.5000
fn.htm:18 0.7311
fn.htm:18 0.8808
fn.htm:18 0.9526
fn.htm:18 0.9820
fn.htm:18 0.9933
fn.htm:18 0.9975
fn.htm:18 0.9991
fn.htm:18 0.9997
fn.htm:18 0.9999
fn.htm:18 1.0000 (990)
```



![image-20230528072506949](02_神经网络如何工作.assets\image-20230528072506949.png)

最终的输出层也是同上，最终得到输出值如图。



### 1.10 学习来自多个节点的权重

关于根据 **输出误差** 调节与连接输入的权重，其中一个思想是

<img src="02_神经网络如何工作.assets\image-20230528130504189.png" alt="image-20230528130504189" style="zoom:33%;" />让所有造成误差的节点 **平分** 误差

<img src="02_神经网络如何工作.assets\image-20230528130544795.png" alt="image-20230528130544795" style="zoom:33%;" />另一种思想是 **按贡献比例** 调整误差





### 1.11 多个输出节点反向传播误差

<img src="02_神经网络如何工作.assets\image-20230528160002029.png" alt="image-20230528160002029" style="zoom:33%;" />

> 如果神经网络具有多个层，那么我们就从最终输出层往回工作，对每一层重复应用相同的思路

我们将第一个输出节点的误差标记为 **e₁** 

即 样本目标值（**t₁**） - 实际输出值（**o₁**）



再根据 W₁.₁ 和 W₂.₁ 的权重比值分别分配新的权重

**W₁.₁ = e₁ · (W₁.₁ / (W₁.₁ +W₂.₁))**

**W₂.₁ = e₁ · (W₂.₁ / (W₁.₁ +W₂.₁))**

对于输出节点二也是同理。



### 1.12 反向传播误差到更多层中

![image-20230528162600811](02_神经网络如何工作.assets\image-20230528162600811.png)

我们将 **隐藏层到输出层** 的输出误差标记为 **e<span style="font-size: 0.5em;">output</span>** 将再输出层和隐藏层之间的链接权重标记为 **w<span style="font-size: 0.5em;">ho</span>**

同理，将 **输入层到隐藏层** 的输出误差标为 **e<span style="font-size: 0.5em;">hidden</span>** 将再输出层和隐藏层之间的链接权重标记为 **w<span style="font-size: 0.5em;">hi</span>**



> 根据上一小节，我们可以算出  **e<span style="font-size: 0.5em;">output</span>**  跟 **w<span style="font-size: 0.5em;">ho</span>** 的值
>
> 但对于隐藏层的节点，我们没有目标值或所希望的输出值

不过我们可以通过分别将隐藏层单个节点到输出层的连接误差相加，算出其总误差



**e<span style="font-size: 0.5em;">hidden_1</span> = 链接W₁.₁ 和 链接W₂.₁ 上的分割误差之和**

![image-20230528164901539](02_神经网络如何工作.assets\image-20230528164901539.png)



![image-20230528165945747](02_神经网络如何工作.assets\image-20230528165945747.png)



### 1.13 使用矩阵乘法进行反向传播误差

> 使用矩阵乘法使运算变得更加简洁，即将过程矢量化（vectorise the process）
>
> 我们可以相对简单地以矩阵形式表达大批量的计算，这有利于我们的书写，
>
> 并且由于这种方法利用了所需计算中的相似性，因此这允许计算机更高效的完成所有计算工作



<img src="02_神经网络如何工作.assets\image-20230528173302774.png" alt="image-20230528173302774" style="zoom:33%;" />根据上个示例的神经网络有两个结点，因此误差分为 e₁ 和 e₂ 。

<img src="02_神经网络如何工作.assets\image-20230528185123560.png" alt="image-20230528185123560" style="zoom:33%;" />再根据隐藏层与输出层间的链接权重比值得到 **e<span style="font-size: 0.5em;">hidden</span>**



> 根据该以上式子，可以观察到，最重要的是**输出误差**与**链接权重**的乘法
>
> 较大的权重就意味着携带较多的输出误差给隐藏层。

- 这些分数的分母是一种**归一化因子**。
- 如果我们忽略了这个因子，那么我们仅仅失去**后馈误差**的大小
- 因此，我们可以使用 **e₁ * w₁.₁** 来代替 **e₁ * w₁.₁ / (w₁.₁ + w₂.₁)**，使矩阵乘法更容易辨认

![image-20230528190004528](02_神经网络如何工作.assets\image-20230528190004528.png)

![image-20230528190256368](02_神经网络如何工作.assets\image-20230528190256368.png)

> 虽然这样做看起来不错，但是将归一化因子切除，做得正确吗？
>
> 实践证明，这种相对简单的误差信号反馈方式，与我们先前相对复杂的方式一样有效



### 1.14 我们实际上如何更新权重

> 前部分讨论的是线性分类器如何使用误差调整权重
>
> 数学太复杂了，因此我们不能使用微妙的代数来直接计算出节点的权重。
>
> 当我们通过网络前馈信号时，有太多的权重需要组合，太多的函数的函数的函数……需要组合。
>
> 即便是一个只有3x3的神经网络，你该如何调整输入层第一个节点和隐层层第二个节点之间链路的权重，
>
> 以使得输出层第三个节点的输出增加0.5呢？
>
> ![image-20230601152201939](02_神经网络如何工作.assets\image-20230601152201939.png)



> 想象一下，，一个非常复杂、有波峰波谷的地形以及连绵的群山峻岭，你知道你在一个上坡上，需要到坡底。
>
> 对于整个地形，你没有精确的地图，只有一把手电，你能做什么呢？
>
> 你可能会使用手电，做近距离的观察。
>
> 当你看到一块土地看起来是下坡，你向这个方向走。通过这种方式，你不需要完整的地图，也不需要事先制定路线。
>
> 一步一个脚印，缓慢地下山。

> 在数学上，这种方法称为**梯度下降**（gradient descent）。
>
> 梯度是指地面的坡度，你走的方向时最陡的坡度向下的方向。

下图是一个简单的函数 y = (x - 1)² + 1

<img src="02_神经网络如何工作.assets\image-20230601153043625.png" alt="image-20230601153043625" style="zoom:25%;" />要应用梯度下降法，我们必须找一个起点。

上图选择了一个随机的起点，该点斜率为负，因此我们沿着x轴向右，向实际最小值靠近了一些。



<img src="02_神经网络如何工作.assets\image-20230601153625777.png" alt="image-20230601153625777" style="zoom:25%;" />如果斜率为正，则沿x轴向左移动

我们可以一直这样操作，直到几乎不能改进则代表找到了最小值。



> 在移动过程中，还要适当改变步子大小，避免超调，造成在最小值的位置反复横跳。

> 使用梯度下降法时，我们一般不使用代数计算出最小值，如果函数一个非常复杂的函数。
>
> 即使不使用数学精确计算出斜率，我们也可以**估计**出斜率。
>
> 当函数有很多参数时，这种方法才真正地显现出它的亮点。
>
> y也许不单单取决于x，y也可能取决于a、b、c、d……。
>
> 记得输出函数吧，神经网络的误差函数取决于写多的权重参数，这些参数通常有数百个呢。

我们使用复杂些的、依赖两个参数的函数进一步说明梯度下降法

<img src="02_神经网络如何工作.assets\image-20230601154622178.png" alt="image-20230601154622178" style="zoom: 50%;" />

如果有多个山谷，梯度下降法有时可能会卡在错误的山谷。

为了避免终止于错误的山谷或错误的函数最小值，我们从山上的不同点开始，多次训练神经网络，

确保并不总是终止于错误的山谷。

![image-20230601154854178](02_神经网络如何工作.assets\image-20230601154854178.png)

> 神经网络本身的输出函数不是一个误差函数，
>
> 但由于误差是目标训练值与实际输出值之间的一个差值，
>
> 因此我们可以很容易地把输出函数变成误差函数。

![image-20230601155146499](02_神经网络如何工作.assets\image-20230601155146499.png)

- 如图是测量方法一，直接将所有误差相加，

  这样会出现正负抵消的情况使误差为0

- 测量方法二是将误差的绝对值相加，

  这种方法可能会有超调的风险

- 测量方法三使用**差的平方**，（目标值 - 实际值）²
  - 使用差的平方，可以很容易使用代数计算出梯度下降的斜率。
  - 误差函数平滑连续，可以使得梯度下降发很好地发挥作用——没有间断，也没有突然的跳跃。
  - 越接近最小值，梯度越小，可以使用这个函数调节补偿，超调的风险会变小



#### 误差函数相对于权重的斜率

这就需要引入 **微积分** 了，详见：<a href="./附录A_微积分简介.md?target=file:md" target="_blank">附录A_微积分简介</a>

<img src="02_神经网络如何工作.assets\image-20230601181544522.png" alt="image-20230601181544522" style="zoom:33%;" />

上图仅演示了一个权重，下图演示了两个链接权重

<img src="02_神经网络如何工作.assets\image-20230601181708478.png" alt="image-20230601181708478" style="zoom:33%;" />

*<font color="#666">当函数具有多个参数是，要画出误差曲面相对较难，但使用梯度下降寻找最小值的思想是相同的。</font>*



我们要取得的目标为



δE / δWⱼ,ₖ

当权重Wⱼ,ₖ改变时，误差E是如何改变的

<img src="02_神经网络如何工作.assets\image-20230601182841703.png" alt="image-20230601182841703" style="zoom:33%;" />

![image-20230610212942465](02_神经网络如何工作.assets\image-20230610212942465.png)

推导过程详见：<a href="我们实际上如何更新权重_推导过程.md">我们实际上如何更新权重_推导过程</a>

